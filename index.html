<!DOCTYPE html>
<html>
  <style type="text/css"> body {height:100%; padding:0; margin:0; width:100%;}
    body {display:flex; flex-direction:column;}
    div.container { width: 900px; margin: 0 auto; text-align: justify; }
    .teaser_vid { width: 900px; margin: 0 auto; text-align: center; }
    .demo_vid { width: 400px; margin: 0 auto; text-align: center;}
    header {min-height:50px; background:green;}
    #video{text-align: center;}
    
    #title{ font-family: Gill Sans, Verdana; font-size: 11px; line-height:14px;
      text-transform: uppercase; letter-spacing: 2px; font-weight: bold; color: #444; }
    div.container p { font-family: Times New Roman,
      times,times-roman, georgia, serif; font-size: 16px; line-height: 20px;
      color: #444; }

    div.container h2 { font-family: times, Times New Roman, times-roman,
      georgia, serif; font-size: 28px; line-height: 40px; letter-spacing:
      -1px;color: #444; }

    div.container h3 { font-family: times, Times New Roman, times-roman,
      georgia, serif; font-size: 18px; line-height: 20px; letter-spacing:
      -1px;color: #444; }
  </style>
  <body>

    <div class="container">

      <hr style="clear:both;"/>
      <H2 style="text-align: center;"> Layered Controllable Video Generation </H2>
      <p style="text-align: center;"><a href="https://scholar.google.com/citations?user=qWU4y1wAAAAJ&hl=en">Jiahui (Gabriel) Huang</a>,
        <a href="https://scholar.google.ca/citations?user=oAYi1YQAAAAJ&hl=en">Yuhe Jin</a>,
        <a href="https://scholar.google.com/citations?user=pr6rIJEAAAAJ&hl=en">Kwang Moo Yi</a>,
        <a href="https://scholar.google.com/citations?user=P2mG6rcAAAAJ&hl=en">Leonid Sigal</a>
        <br>
        <br>
        The University of British Columnbia
      </p>

      <div class="container">
        <hr style="clear:both;"/>
          <div id="title">Abstract:</div>
          <p> We introduce layered controllable video generation, where we, without any supervision, 
              decompose the initial frame of a video into foreground and background layers, 
              with which the user can control the video generation process by simply manipulating the foreground mask.
              This video is a quick abstract of our method.
              Please see our <a href="https://arxiv.org/abs/2111.12747">paper</a> for more details.
          </p>
      </div>

      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/video_abstract.mov" type="video/mp4">
          Your browser does not support HTML5 video.  To see this video, please open
          video_abstract.mp4 (see bellow for details on the codec to use).
      </video>

    </div>
    <hr style="clear:both;"/>
    <hr style="clear:both;"/>


    <div class="container">
      <div id="title">Real-time demo:</div>
        <p> We show a screen recording of a real-time demo of our model reacting to a user's inputs.
            In this demo, we show that the user can specify not only the directions of 
            the movements, but also the speed.
            It proves that our model is able to take in continous control parameteres rather than a set of discreat ones.
        </p>
    </div>

    <video class="demo_vid" autoplay loop muted>
      <source src="videos/demo.mp4" type="video/mp4">
        Your browser does not support HTML5 video.  To see this video, please open
        video_abstract.mp4 (see bellow for details on the codec to use).
    </video>

    <hr style="clear:both;"/>
    <hr style="clear:both;"/>
    <hr style="clear:both;"/>

    <div class="container">
      <div id="title">Qualitative Comparisions:</div>
        <p> Here we compare the videos generated by our methods against videos generated by other baselines 
          (SAVP[<b><span style="color: #00ff00">35</span></b>], 
          CADDY[<b><span style="color: #00ff00">37</span></b>], and  
          MoCoGAN[<b><span style="color: #00ff00">47</span></b>]) on both BAIR and Tennis dataset.
          We show results of all three variants of our model (see paper for more details).
        </p>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/bair_compare.mov" type="video/mp4" />
          Your browser does not support HTML5 video.
      </video>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/tennis_compare.mov" type="video/mp4" />
          Your browser does not support HTML5 video.
      </video>
      <p>
        Most noticeably, on both datasets, all three variants of our model successfully generated motions that consistantly correspond to the ground truth sequences,
        where other baselines failed to do so to some degree. In addition, in terms of image quality, our method is superior to competitors.
      </p>
    </div>

    <hr style="clear:both;"/>

    <div class="container">
      <div id="title">Controlled Generation:</div>
        <p> Here we show how our model reacts to user control signals. For each sample, we keep applying the same
            moving control signal of the direction that the arrows are pointing. Notice how the generated motions
            precisely follow the directions of the arrows.
        </p>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/control.mov" type="video/mp4" />
          Your browser does not support HTML5 video.  To see this video, please
          open teaser.mp4 (see bellow for details on the codec
          to use).
      </video>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/control_bair.mov" type="video/mp4" />
          Your browser does not support HTML5 video.  To see this video, please
          open teaser.mp4 (see bellow for details on the codec
          to use).
      </video>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/control_tennis.mov" type="video/mp4" />
          Your browser does not support HTML5 video.  To see this video, please
          open teaser.mp4 (see bellow for details on the codec
          to use).
      </video>
    </div>

    <div class="container">
      <div id="title">Other Applications:</div>
      <p>
        Action Mimicking: Our method can be used to extract the motion from a driving sequence and then apply onto different appearances (staring frames). 
      </p>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/mimic.mov" type="video/mp4" />
          Your browser does not support HTML5 video.  To see this video, please
          open teaser.mp4 (see bellow for details on the codec
          to use).
      </video>
      <p>
        Frame Animation: We can animate a single input image with a variety of different motions.
      </p>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/frame_animation.mov" type="video/mp4" />
          Your browser does not support HTML5 video.  To see this video, please
          open teaser.mp4 (see bellow for details on the codec
          to use).
      </video>
      <p>
        Multi-Object Animation: our method is capable of generating and controlling videos with multiple moving objects by simply 
        overlaying two individually controlled mask sequences together.
      </p>
      <video class="teaser_vid" autoplay loop muted>
        <source src="videos/multi_player.mov" type="video/mp4" />
          Your browser does not support HTML5 video.  To see this video, please
          open teaser.mp4 (see bellow for details on the codec
          to use).
      </video>

    </div>


<div class="container">
  <hr style="clear:both;"/>
  <!-- <div id="title">Supplementary PDF:</div>
  <p>
  	Supplementary PDF is available <a href="appendix.pdf">here</a>, where we present the impelementation and training details
    of our model, as well as the disscussion on the broader impacts of our work.
  </p> -->

  <!-- <hr style="clear:both;"/> -->
  <div id="title">Reference:</div>
  <p style="text-indent:-25px; margin-left:25px; text-align: left;">
    [<b><span style="color: #00ff00">35</span></b>]&nbsp&nbspAlex X. Lee,
    Richard Zhang, Frederik Ebert, P.Abbeel, Chelsea Finn, and Sergey Levine. "Stochastic
    adersarial video prediction."  &nbsp&nbspArXiv preprint, 2018.
  </p>
  <p style="text-indent:-25px; margin-left:25px; text-align: left;">
    [<b><span style="color: #00ff00">37</span></b>]&nbsp&nbspWilli Menapace, Stephane Lathuiliere,
    S. Tulyakov, Aliaksandr Siarohin, and Elisa Ricci. "Playable Video Generation." CVPR, 2021
  </p>
  <p style="text-indent:-25px; margin-left:25px; text-align: left;">
    [<b><span style="color: #00ff00">47</span></b>]&nbsp&nbspSergey Tulyakov, Ming-Yu Liu,
    Xiaodong Yang, and Jan Kautz. "MoCoGAN: Decomposing motion and content for video generation." &nbsp&nbspCVPR, 2018
  </p>
  <!-- <hr style="clear:both;"/>
  <p style="text-align:center;">
    The supplementary videos are encoded by FFMPEG with h.264 codec. </br> If
    you can't play the video, please download the VLC player
    at: <a href="http://www.videolan.org/vlc/index.html">http://www.videolan.org/vlc/index.html</a>
  </p> -->

</div>

</body>
</html>
